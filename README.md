# VibeThinker
<p align="center"><img src="./figures/logo.png" width="100"/></p>

<p align="center">ðŸ¤— <a href="https://huggingface.co/WeiboAI">Hugging Face</a>&nbsp&nbsp


## Introduction

VibeThinker-1.5B is a 1.5B-parameter dense model that challenges the prevailing notion that small models inherently lack robust reasoning capabilities. Developed with an innovative post-training methodology centered on the **"Spectrum-to-Signal Principle (SSP)"**, our model achieves reasoning performance comparable to models hundreds of times larger.

<p align="center"><img src="./figures/vt1.5b_eval.png" /></p>

## Key Features

- **Ultra-Efficient**: Only 1.5B parameters - 100Ã— to 600Ã— smaller than giants like Kimi K2 (1000B+) and DeepSeek R1 (671B)
- **Cost-Effective**: Total training cost of **$7,800**
- **State-of-the-Art Performance**: Superior reasoning capabilities in mathematical and coding tasks
- **Innovative Methodology**: SSP framework with Two-Stage Diversity-Exploring Distillation and MaxEnt-Guided Policy Optimization

## Model Downloads

The model checkpoint is available at: [Hugging Face Hub](xxx)


## License

This code repository is licensed under [the MIT License](https://github.com/WeiboAI/VibeThinker/blob/main/LICENSE).

## Citation

If you find our work helpful, feel free to give us a cite.

```

```